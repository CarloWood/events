Non-Blocking Critical Area.

How to create an area that doesn't keep a mutex locked while inside it,
but that nevertheless is a critical area (only one thread at a time
is allowed in that area)?

The reason for not locking a mutex is because we don't want to block
threads that try to enter the area.

There are basically two ways to achieve this:

1) Using a mutex and try_lock.

  if (m.try_lock())
  {
    // Non-blocking critical area.
    m.unlock();
  }

2) Using an atomic counter.

  if (counter.fetch_add(1) == 0)
  {
    // Non-blocking critical area.
  }
  counter.fetch_sub(1);

The latter is clearly more flexible as it also counts the threads that
failed to enter the non-blocking area. So, we'll use the latter.

If a thread can't enter the critical area it should queue the event in
a container. Concurrent access to this container can be prevented with
a mutex; aka:

  void handle(TYPE const& data)
  {
    if (m_busy_depth.fetch_add(1) == 0)
    {
      // Non-blocking critical area.
      m_callback(data);
    }
    else
    {
      // Queue sequence.
      std::lock_guard<std::mutex> lock(m_events_mutex);
      m_events.push_back(data);
    }
    m_busy_depth.fetch_sub(1);
  }

Of course, those queued events need to be processed too; they need to
be removed from the queue and passed to m_callback (again without any mutex
being locked). So that a part of the code will look something like this:

    // Pop and callback sequence.
    {
      std::lock_guard<std::mutex> lock(m_events_mutex);
      data = m_events.front();
      m_events.pop_front();
    }
    m_callback(data);           // At this point m_busy_depth must be larger than 0.

The reason that m_busy_depth must be larger than 0 at that point is because we
don't want other threads to enter the non-blocking critical area at the same
time. Moreover we can't call m_callback ourselves here when there is already
some thread in that area.

When we reach the m_busy_depth.fetch_sub(1) and there is any other thread above
us then it doesn't make sense to even try and execute the pop and callback
sequence: if that other thread is in the non-blocking area then we aren't
allowed to call m_callback, and we can't wait for it because we don't know how
long such a callback takes; we would be blocking if we waited. While if that
other thread is in the Queue sequence area then likely it has the lock on the
mutex and we'd only be blocking until also that thread reached the
m_busy_depth.fetch_sub(1) line. More importantly however: it is entirely safe
to do nothing *because* there is still a thread above us (that is likely
blocking us from doing the Pop and callback sequence anyway): we'll just leave
it to that thread to empty the queue.

Therefore, the Pop and callback sequence should be put in an area that is only
executed when there are no more threads above us:

  void handle(TYPE const& type)
  {
    TYPE data = type;
    for (;;)
    {
      if (m_busy_depth.fetch_add(1) > 0)
      {
        // Queue sequence.
        std::lock_guard<std::mutex> lock(m_events_mutex);
        m_events.push_back(data);
      }
      else
      {
        // Non-blocking critical area.
        m_callback(data);
      }
      if (m_busy_depth.fetch_sub(1) > 1)    // Are there any threads above us?
        break;
      // Pop and callback sequence.
      std::lock_guard<std::mutex> lock(m_events_mutex);
      if (m_events.empty())
        break;
      data = m_events.front();
      m_events.pop_front();
    }
  }

I believe this code actually works but it has some issues.

First of all, m_callback() can't throw an exception (theoretically
also the push_back - ie, out of memory - but that is extremely
much more unlikely and in most cases considered an unrecoverable
error anyway). If m_callback() would throw an exception then that
thread leaves this function without decrementing the m_busy_depth
count, causing no thread to ever enter the Non-blocking critical
area anymore: every subsequent event would be queued. Also, the
code that causes an event and calls handle should be decoupled
from clients requesting callbacks. If a callback would throw an
exception then there is reason to assume the caller here can
even know about it, let alone handle it.

Secondly, this makes unnecessary copies of 'type'.

There is another complication; this handle() function is a member
function of a Request<TYPE> object that represents the request of
some client object to receive callbacks for a given TYPE of event.
The Request<TYPE> represents the m_callback. However, m_busy_depth
can be thought of as the 'mutex' protecting the whole of the client
object against concurrent accesses. m_busy_depth has to be a member
of BusyInterface object that has a 1 on 1 relationship with this
client object, while the client object might do any number of
requests for events, even of different TYPEs.

This BusyInterface object could take the following form:

  struct BusyInterface
  {
   private:
    std::atomic_uint m_busy_depth;
    std::mutex m_events_mutex;
    std::deque<QueuedEventBase const*> m_events;

   public:
    BusyInterface() : m_busy_depth(0) { }
    bool set_busy() { return m_busy_depth.fetch_add(1) == 0; }
    bool unset_busy() { return m_busy_depth.fetch_sub(1) == 1; }

    void push(QueuedEventBase const* new_queued_event)
    {
      std::lock_guard<std::mutex> lock(m_events_mutex);
      m_events.push_back(new_queued_event);
    }

    QueuedEventBase const* pop()
    {
      QueuedEventBase const* queued_event = nullptr;
      std::lock_guard<std::mutex> lock(m_events_mutex);
      if (!m_events.empty())
      {
        queued_event = m_events.front();
        m_events.pop_front();
      }
      return queued_event;
    }
  };

If then we rename our handle() function to Request<TYPE>::handle_bi(),
and give the Request<TYPE> object a pointer to this BusyInterface
(that was passed by the client object at the moment it made the request)
we can almost literally translate the above handle() function into:

  template<typename TYPE>
  void Request<TYPE>::handle_bi(TYPE const& type)
  {
    QueuedEventBase const* data = &type;      // Assume this works.
    for (;;)
    {
      if (!m_busy_interface->set_busy())
      {
        // Queue sequence.
        m_busy_interface->push(data);
      }
      else
      {
        // Non-blocking critical area.
        m_callback(data);
      }
      if (!m_busy_interface->unset_busy())    // Are there any threads above us?
        break;
      // Pop and callback sequence.
      data = m_busy_interface->pop();
      if (!data)
        break;
    }
  }

One of the many problems with this is is that events pushed to the queue
might be of a different TYPE than TYPE in the Request<TYPE::handle_bi
that pops them. So, the 'data' that was popped at the bottom might not
need to be passed to the m_callback of *this* Request object. What is
needed is to push a pointer to the Request object itself (and thus the
m_callback) to the queue as well.

All in all, it means that 'data' pointers popped from the queue need to
be handled differently than the one that we enter the function with.
Therefore lets start with rewritting the above function as follows,
where we unwrap the loop for the first call to set_busy() + queue or
callback:

  template<typename TYPE>
  void Request<TYPE>::handle_bi(TYPE const& type)
  {
    QueuedEventBase const* data = &type;      // Assume this works.
    if (!m_busy_interface->set_busy())
    {
      // Queue sequence.
      m_busy_interface->push(data);
    }
    else
    {
      // Non-blocking critical area.
      m_callback(data);
    }
    while (m_busy_interface->unset_busy())    // Are we the last thread to leave this function?
    {
      // Pop and callback sequence.
      data = m_busy_interface->pop();
      if (!data)
        break;
      if (!m_busy_interface->set_busy())
      {
        // Queue sequence.
        m_busy_interface->push(data);
      }
      else
      {
        // Non-blocking critical area.
        m_callback(data);
      }
    }
  }

Which is the exact same code, but written differently.  And then make
the following changes:

1) Lets call this function with a pointer instead of a reference.
2) Allocate QueuedEvent<TYPE> objects, derived from a common base QueuedEventBase,
   and push the pointers to the base class onto the queue. Also store a pointer
   to the Request<TYPE> object in this object.

This allows us to solve the problem that the queue contains events for
different TYPEs.

  template<typename TYPE>
  void Request<TYPE>::handle_bi(TYPE const* data)
  {
    if (!m_busy_interface->set_busy())
    {
      // Queue sequence.
      QueuedEventBase const* new_queued_event = new QueuedEvent<TYPE>(this, *data);
      m_busy_interface->push(new_queued_event);
    }
    else
    {
      // Non-blocking critical area.
      m_callback(data);
    }
    while (m_busy_interface->unset_busy())    // Are we the last thread to leave this function?
    {
      // Pop and callback sequence.
      QueuedEventBase const* queued_event = m_busy_interface->pop();
      if (!queued_event)
        break;
      if (!m_busy_interface->set_busy())
      {
        // Re-queue sequence.
        m_busy_interface->push(queued_event);
      }
      else
      {
        // Non-blocking critical area for handling previously queued events.
        queued_event->rehandle();             // Call QueuedEvent<TYPE>::rehandle() which calls Request<TYPE>::m_callback(queued_data) for the correct TYPE.
        delete queued_event;
      }
    }
  }

Where QueuedEventBase and QueuedEvent<TYPE> are defined as

  class QueuedEventBase
  {
   public:
    virtual ~QueuedEventBase() { }
    virtual void rehandle() const = 0;
  };

  template<typename TYPE>
  struct QueuedEvent : QueuedEventBase
  {
    Request<TYPE>* m_request;
    TYPE m_data;

    QueuedEvent(Request<TYPE>* request, TYPE const& data) : m_request(request), m_data(data) { }

   private:
    void rehandle() const override
    {
      m_request->m_callback(m_data);
    }
  };

Note that in the real code a memory pool is used for the QueuedEvent objects,
instead of a call to new.

